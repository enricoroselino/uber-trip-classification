{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Uber Trip Classification EDA and Modeling\n",
    "Oleh : [enricoroselino](https://www.linkedin.com/in/enricoroselino/)\n",
    "\n",
    "Dataset : [DTS Google Tensorflow 2 Demo day - GGU](https://www.kaggle.com/datasets/mnavas/taxi-routes-for-mexico-city-and-quito/code)\n",
    "\n",
    "Tujuan :\n",
    "* Membuat model yang dapat memprediksi kesalahan dalam pencatatan data perjalanan Uber.\n",
    "\n",
    "Masalah :\n",
    "* Tidak ada kolom label untuk menjadi target.\n",
    "* Data tanggal tidak ada notasi AM / PM.\n",
    "\n",
    "Landasan Teori :\n",
    "* Batas kecepatan di kawasan perkotaan adalah 50 km/jam, kawasan permukiman adalah 30 km/jam, kawasan jalanan antar kota paling rendah adalah 60 km/jam dan kecepatan tol dalam kota paling cepat adalah 80 km/jam. ([Kumparan](https://kumparan.com/info-otomotif/batas-kecepatan-untuk-dalam-kota-begini-aturannya-1xvq35hLvXP/3), [Otomotif Kompas](https://otomotif.kompas.com/read/2022/06/20/191100215/batas-kecepatan-berkendara-di-jalan-tol-tidak-semua-sama))\n",
    "* Waktu tunggu maksimal adalah 5 menit. ([therideshareguy](https://therideshareguy.com/uber-extends-wait-time/))\n",
    "\n",
    "\n",
    "Label Feature Description :\n",
    "* 0 = Trip not valid\n",
    "* 1 = Trip valid\n",
    "\n",
    "Kesimpulan :\n",
    "* Dapat disimpulkan model **Deep Neural Network** lebih andal dengan **f1 score : 56.37%** pada data test dibandingkan dengan model lainnya. Dengan tingkat false positive yang cukup rendah 0.18 dan false negative 0.42, model dapat diandalkan karena **lebih sedikit kesalahan** memprediksi perjalanan yang **sebenarnya valid dan tidak valid**.\n",
    "\n",
    "* Sebagai alternatif, model **Random Forest Classifier** memiliki **f1 score : 54.42%** pada data test. Dengan pemodelan yang lebih ringkas namun performanya tidak jauh berbeda dengan Deep Neural Network. Model ini menghasilkan **false positive yang lebih sedikit**, yang berarti akan **minim kesalahan penagihan** dan false negative dapat diverifikasi kembali total tagihan yang harus dibayar oleh customer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Libs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32md:\\code\\python-codes\\uber-trip-classification\\uber_trip_classification.ipynb Cell 3\u001b[0m in \u001b[0;36m<cell line: 7>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/code/python-codes/uber-trip-classification/uber_trip_classification.ipynb#W3sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mdatetime\u001b[39;00m \u001b[39mimport\u001b[39;00m date, datetime\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/code/python-codes/uber-trip-classification/uber_trip_classification.ipynb#W3sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mnumpy\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mnp\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/d%3A/code/python-codes/uber-trip-classification/uber_trip_classification.ipynb#W3sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mmatplotlib\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpyplot\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mplt\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/code/python-codes/uber-trip-classification/uber_trip_classification.ipynb#W3sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mseaborn\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39msns\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/code/python-codes/uber-trip-classification/uber_trip_classification.ipynb#W3sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mgeopy\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdistance\u001b[39;00m \u001b[39mimport\u001b[39;00m distance\n",
      "File \u001b[1;32md:\\code\\pythonenv\\directml310\\lib\\site-packages\\matplotlib\\pyplot.py:57\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     55\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mmatplotlib\u001b[39;00m \u001b[39mimport\u001b[39;00m docstring\n\u001b[0;32m     56\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mmatplotlib\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mbackend_bases\u001b[39;00m \u001b[39mimport\u001b[39;00m FigureCanvasBase, MouseButton\n\u001b[1;32m---> 57\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mmatplotlib\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mfigure\u001b[39;00m \u001b[39mimport\u001b[39;00m Figure, figaspect\n\u001b[0;32m     58\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mmatplotlib\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mgridspec\u001b[39;00m \u001b[39mimport\u001b[39;00m GridSpec, SubplotSpec\n\u001b[0;32m     59\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mmatplotlib\u001b[39;00m \u001b[39mimport\u001b[39;00m rcParams, rcParamsDefault, get_backend, rcParamsOrig\n",
      "File \u001b[1;32md:\\code\\pythonenv\\directml310\\lib\\site-packages\\matplotlib\\figure.py:25\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mnumpy\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mnp\u001b[39;00m\n\u001b[0;32m     24\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mmatplotlib\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mmpl\u001b[39;00m\n\u001b[1;32m---> 25\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mmatplotlib\u001b[39;00m \u001b[39mimport\u001b[39;00m _blocking_input, docstring, projections\n\u001b[0;32m     26\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mmatplotlib\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39martist\u001b[39;00m \u001b[39mimport\u001b[39;00m (\n\u001b[0;32m     27\u001b[0m     Artist, allow_rasterization, _finalize_rasterization)\n\u001b[0;32m     28\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mmatplotlib\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mbackend_bases\u001b[39;00m \u001b[39mimport\u001b[39;00m (\n\u001b[0;32m     29\u001b[0m     FigureCanvasBase, NonGuiException, MouseButton, _get_renderer)\n",
      "File \u001b[1;32md:\\code\\pythonenv\\directml310\\lib\\site-packages\\matplotlib\\projections\\__init__.py:55\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[39mNon-separable transforms that map from data space to screen space.\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     52\u001b[0m \u001b[39m`matplotlib.projections.polar` may also be of interest.\u001b[39;00m\n\u001b[0;32m     53\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m---> 55\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39m.\u001b[39;00m \u001b[39mimport\u001b[39;00m axes, docstring\n\u001b[0;32m     56\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mgeo\u001b[39;00m \u001b[39mimport\u001b[39;00m AitoffAxes, HammerAxes, LambertAxes, MollweideAxes\n\u001b[0;32m     57\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mpolar\u001b[39;00m \u001b[39mimport\u001b[39;00m PolarAxes\n",
      "File \u001b[1;32md:\\code\\pythonenv\\directml310\\lib\\site-packages\\matplotlib\\axes\\__init__.py:1\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39m_subplots\u001b[39;00m \u001b[39mimport\u001b[39;00m \u001b[39m*\u001b[39m\n\u001b[0;32m      2\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39m_axes\u001b[39;00m \u001b[39mimport\u001b[39;00m \u001b[39m*\u001b[39m\n",
      "File \u001b[1;32md:\\code\\pythonenv\\directml310\\lib\\site-packages\\matplotlib\\axes\\_subplots.py:4\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mmatplotlib\u001b[39;00m \u001b[39mimport\u001b[39;00m _api, cbook\n\u001b[0;32m      3\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mmatplotlib\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39maxes\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39m_axes\u001b[39;00m \u001b[39mimport\u001b[39;00m Axes\n\u001b[1;32m----> 4\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mmatplotlib\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mgridspec\u001b[39;00m \u001b[39mimport\u001b[39;00m GridSpec, SubplotSpec\n\u001b[0;32m      7\u001b[0m \u001b[39mclass\u001b[39;00m \u001b[39mSubplotBase\u001b[39;00m:\n\u001b[0;32m      8\u001b[0m     \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m      9\u001b[0m \u001b[39m    Base class for subplots, which are :class:`Axes` instances with\u001b[39;00m\n\u001b[0;32m     10\u001b[0m \u001b[39m    additional methods to facilitate generating and manipulating a set\u001b[39;00m\n\u001b[0;32m     11\u001b[0m \u001b[39m    of :class:`Axes` within a figure.\u001b[39;00m\n\u001b[0;32m     12\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:1027\u001b[0m, in \u001b[0;36m_find_and_load\u001b[1;34m(name, import_)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:1002\u001b[0m, in \u001b[0;36m_find_and_load_unlocked\u001b[1;34m(name, import_)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:945\u001b[0m, in \u001b[0;36m_find_spec\u001b[1;34m(name, path, target)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap_external>:1439\u001b[0m, in \u001b[0;36mfind_spec\u001b[1;34m(cls, fullname, path, target)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap_external>:1411\u001b[0m, in \u001b[0;36m_get_spec\u001b[1;34m(cls, fullname, path, target)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap_external>:1544\u001b[0m, in \u001b[0;36mfind_spec\u001b[1;34m(self, fullname, target)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap_external>:147\u001b[0m, in \u001b[0;36m_path_stat\u001b[1;34m(path)\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "import math\n",
    "import re\n",
    "import pandas as pd\n",
    "from datetime import date, datetime\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from geopy.distance import distance\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from imblearn.combine import SMOTETomek\n",
    "from fast_ml.model_development import train_valid_test_split\n",
    "\n",
    "%matplotlib inline\n",
    "sns.set()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bog_path = os.path.join(\"dataset\", \"bog_clean.csv\")\n",
    "mex_path = os.path.join(\"dataset\", \"mex_clean.csv\")\n",
    "uio_path = os.path.join(\"dataset\", \"uio_clean.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bog_df = pd.read_csv(bog_path)\n",
    "mex_df = pd.read_csv(mex_path)\n",
    "uio_df = pd.read_csv(uio_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(bog_df.shape)\n",
    "print(mex_df.shape)\n",
    "print(uio_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check bog unique value\n",
    "for i in bog_df[[\"id\", \"vendor_id\", \"store_and_fwd_flag\"]] :\n",
    "    print(f\"{i} : {bog_df[i].nunique()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check mex unique value\n",
    "for i in mex_df[[\"id\", \"vendor_id\", \"store_and_fwd_flag\"]] :\n",
    "    print(f\"{i} : {mex_df[i].nunique()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check uio unique value\n",
    "for i in uio_df[[\"id\", \"vendor_id\", \"store_and_fwd_flag\"]] :\n",
    "    print(f\"{i} : {uio_df[i].nunique()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dapat disimpulkan bahwa tidak ada dupilkat dari kolom `id`, `store_and_fwd_flag` hanya memiliki satu nilai unik dan `datetime` masih dalam tipe data objek pada setiap dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bog_df[\"country\"] = \"colombia\"\n",
    "mex_df[\"country\"] = \"mexico\"\n",
    "uio_df[\"country\"] = \"equador\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uber_df = pd.concat([bog_df, mex_df, uio_df], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uber_df = uber_df.drop([\"id\", \"store_and_fwd_flag\"], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uber_df.describe().T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Terdapat nilai yang tidak realistis minimal dan maksimal pada `trip_duration`, `dist_meters`, dan `wait_sec\t`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"section-one\"></a>\n",
    "### Visualisasi Data\n",
    "\n",
    "Membuat visualisasi untuk menjelaskan sebaran data yang tidak realistis / bermasalah"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(9, 7))\n",
    "sns.scatterplot(\n",
    "    data=uber_df,\n",
    "    x=uber_df.dist_meters/1000,\n",
    "    y=uber_df.trip_duration/3600,\n",
    "    s=100,\n",
    "    alpha=0.85\n",
    ")\n",
    "\n",
    "plt.title(\"Durasi vs Jarak Tempuh Trip\",\n",
    "    loc=\"right\",\n",
    "    fontweight=\"bold\",\n",
    "    size=15\n",
    ")\n",
    "\n",
    "plt.xlabel(\"KM\")\n",
    "plt.ylabel(\"jam\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Waktu dan jarak tempuh seharusnya tidak ada yang negatif, waktu tempuh paling lama adalah 1.94e+4 jam, serta jarak tempuh paling lama adalah 2.15e+6 KM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(9, 7))\n",
    "sns.violinplot(x=uber_df[\"wait_sec\"]/3600)\n",
    "\n",
    "plt.title(\n",
    "    \"Distribusi Waktu Tunggu Driver\",\n",
    "    loc=\"right\",\n",
    "    fontweight=\"bold\",\n",
    "    size=15\n",
    ")\n",
    "plt.xlabel(\"jam\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Waktu driver menunggu penumpang juga terlampau lama yaitu 2.6e+7 jam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate speed in km/h\n",
    "speed_kmph = (uber_df[\"dist_meters\"] / 1000) / (uber_df[\"trip_duration\"] / 3600)\n",
    "speed_kmph.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot kmph distribution\n",
    "plt.figure(figsize=(9, 7))\n",
    "sns.violinplot(x=speed_kmph)\n",
    "\n",
    "plt.title(\n",
    "    \"Distribusi Kecepatan Rata-Rata Trip\",\n",
    "    loc=\"right\",\n",
    "    fontweight=\"bold\",\n",
    "    size=15\n",
    ")\n",
    "plt.xlabel(\"km / jam\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kecepatan kendaraan juga terdapat data negatif dan yang paling cepat adalah 5.94e+06"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mengubah format date and time\n",
    "uber_df[\"pickup_datetime\"] = pd.to_datetime(uber_df[\"pickup_datetime\"], format=\"%Y/%m/%d %H:%M:%S\")\n",
    "uber_df[\"dropoff_datetime\"] = pd.to_datetime(uber_df[\"dropoff_datetime\"], format=\"%Y/%m/%d %H:%M:%S\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check max and minimum time\n",
    "print(\"pickup & dropoff maximum time is {} {}\".format(\n",
    "    uber_df[\"pickup_datetime\"].dt.time.max(), \n",
    "    uber_df[\"dropoff_datetime\"].dt.time.max())\n",
    ")\n",
    "\n",
    "print(\"pickup & dropoff minimum time is {} {}\".format(\n",
    "    uber_df[\"pickup_datetime\"].dt.time.min(), \n",
    "    uber_df[\"dropoff_datetime\"].dt.time.min())\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Waktu perjalanan saat diterima dan selesai tidak dalam format 24H serta tidak terdapat notasi AM / PM "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CLEANING, PREPROCESSING, FEATURE ENGINEERING"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *_datetime\n",
    "Menghilangkan time dari tanggal untuk menghindari kesalahan dalam kalkulasi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uber_df[\"pickup_date\"] = pd.to_datetime(uber_df[\"pickup_datetime\"]).dt.date\n",
    "uber_df[\"dropoff_date\"] = pd.to_datetime(uber_df[\"dropoff_datetime\"]).dt.date\n",
    "uber_df = uber_df.drop([\"pickup_datetime\", \"dropoff_datetime\"], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Menghitung lama perjalanan berdasarkan hari"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def day_delta(df) :\n",
    "    day = []\n",
    "    for i in range(len(df)) :\n",
    "        delta = (df.dropoff_date[i] - df.pickup_date[i]).days\n",
    "        day.append(abs(int(delta)))\n",
    "    return day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uber_df[\"day_delta\"] = day_delta(uber_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uber_df = uber_df.drop([\"pickup_date\", \"dropoff_date\"], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### est_meters\n",
    "Membuat estimasi jarak menyetir sesungguhnya, bukan jarak antar 2 titik koordinat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def geodesic(p_lon, p_lat, d_lon, d_lat) :\n",
    "    # calculate distance using geodesic method\n",
    "    # COEF is a coeficient for calibrating the geodesic result to nearly matches OSRM driving distance\n",
    "    # distance in meters\n",
    "    COEF = 1.5165\n",
    "    pickup = (p_lat, p_lon)\n",
    "    dropoff = (d_lat, d_lon)\n",
    "    result = distance(pickup, dropoff).km\n",
    "    return result * COEF * 1000\n",
    "\n",
    "def distance_estimator(df) :\n",
    "    # calculate the duration then append to est_duration\n",
    "    # using geodesic\n",
    "    # name the estimated distance to est_meters\n",
    "    distance = []\n",
    "    for i in range(len(df)) :\n",
    "        PICKUP_LONG = df.pickup_longitude[i]\n",
    "        PICKUP_LAT = df.pickup_latitude[i]\n",
    "        DROPOFF_LONG = df.dropoff_longitude[i]\n",
    "        DROPOFF_LAT = df.dropoff_latitude[i]\n",
    "        result = geodesic(PICKUP_LONG, PICKUP_LAT, DROPOFF_LONG, DROPOFF_LAT)\n",
    "        distance.append(math.ceil(result))\n",
    "    return distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uber_df[\"est_meters\"] = distance_estimator(uber_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### est_duration\n",
    "Membuat estimasi durasi perjalanan sesungguhnya\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def duration_estimator(df) :\n",
    "    # rata-rata kecepatan 40 km/h\n",
    "    time = []\n",
    "    v = 40 * (1000/3600) # average speed in m/s\n",
    "    for i in range(len(df)) :\n",
    "        d = df.est_meters[i]\n",
    "        t = d / v # time travel in seconds\n",
    "        time.append(math.ceil(t))\n",
    "    return time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uber_df[\"est_duration\"] = duration_estimator(uber_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### avg_kmph\n",
    "Membuat estimasi rata-rata kecepatan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def avg_kmph(df) :\n",
    "    speed = []\n",
    "    for i in range(len(df)) : \n",
    "        METERS = df.dist_meters[i]\n",
    "        DURATION = df.trip_duration[i]\n",
    "        result = (METERS / 1000) / (DURATION / 3600)\n",
    "        speed.append(round(abs(result), 4))\n",
    "    return speed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uber_df[\"avg_kmph\"] = avg_kmph(uber_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### diff_meters & diff_duration\n",
    "Menghitung perbedaan data estimasi dan tercatat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def diff(df) :\n",
    "    meters = []\n",
    "    duration = []\n",
    "    for i in range(len(df)) : \n",
    "        EST_METERS = abs(df.est_meters[i])\n",
    "        RECORDED_METERS = abs(df.dist_meters[i])\n",
    "        EST_DURATION = abs(df.est_duration[i])\n",
    "        RECORDED_DURATION = abs(df.trip_duration[i])\n",
    "        result_meters = RECORDED_METERS - EST_METERS\n",
    "        result_duration = RECORDED_DURATION - EST_DURATION\n",
    "        meters.append(abs(result_meters))\n",
    "        duration.append(abs(result_duration))\n",
    "    return meters, duration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uber_df[\"diff_meters\"], uber_df[\"diff_duration\"] = diff(uber_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### vendor_id\n",
    "Menetapkan tipe layanan uber"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uber_df[\"vendor_id\"].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Terdapat beberapa nama layanan yang tidak tersedia pada laman website uber akan dijadikan taxi dan sisanya akan disesuaikan dengan layanan ekivalennya"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def services_extractor(df) :\n",
    "    # extract services name from vendor_id and map the services based in 2022\n",
    "    # some normal services is not available in 2022, it'll be taxi service instead\n",
    "    # uberangel is exclusive to colombia, it'll be uberblack service instead\n",
    "    # ubersuv will be uberxl\n",
    "    SERVICE_NAME = re.compile(\n",
    "        r\"taxi|uberxl|uberx|uberblack|ubervan|uberangel|ubersuv\"\n",
    "    )\n",
    "    df[\"vendor_id\"] = df[\"vendor_id\"].str.lower()\n",
    "    service = []\n",
    "    for i in range(len(df)) :\n",
    "        extract = SERVICE_NAME.search(df.vendor_id[i])\n",
    "        if extract != None :\n",
    "            ext_group = extract.group()\n",
    "            if (ext_group  == \"ubervan\") or (ext_group == \"ubersuv\"):\n",
    "                service.append(\"uberxl\")\n",
    "            elif ext_group == \"uberangel\" :\n",
    "                service.append(\"uberblack\")\n",
    "            else : \n",
    "                service.append(ext_group)\n",
    "        else :\n",
    "            service.append(\"taxi\")\n",
    "    return service"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uber_df[\"service\"] = services_extractor(uber_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uber_df[\"service\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uber_df = uber_df.drop(\"vendor_id\", axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DEFINE TARGET VARIABLE / LABELING\n",
    "Dalam project Uber Trip Classification bertujuan untuk melakukan prediksi terhadap kesalahan aplikasi saat menyimpan data perjalanan yang diakibatkan driver lupa mengakhiri perjalanan pada aplikasi, dan mengakibatkan kesalahan dalam penagihan harga kepada customer.\n",
    "\n",
    "Variabel `est_meters`, `est_duration` dapat dijadikan pembanding kebenaran terhadap variabel `dist_meters` dan `trip_duration` yang terdapat kesalahan dalam peyimpanan.\n",
    "\n",
    "Perbandingan menggunakan batas atas dan batas bawah yang terdiri dari toleransi perjalanan lebih lama atau lebih cepat yang diakibatkan oleh kecepatan mobil dan keadaan lalu lintas yang tidak menentu, batas waktu driver menunggu adalah 5 menit, minimal jarak perjalanan yang dianggap valid adalah 1 KM, serta perjalanan yang terhitung 1 hari mungkin valid apabila terjadi sekitar tengah malam, tetapi >= 2 hari sudah pasti tidak valid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def labeler(df) :\n",
    "    label = []\n",
    "    DIST_MIN = 1000\n",
    "    WT = 5 * 60\n",
    "    for i in range(len(df)) :\n",
    "        DLB = df.est_meters[i] * 0.8 # might be closer\n",
    "        DHB = df.est_meters[i] * 1.5 # might be further\n",
    "        TLB = df.est_duration[i] * 0.6667 # might be faster (~ 40 km/h - 60 km/h)\n",
    "        THB = df.est_duration[i] * 4 * 1.5 # might be slower (~ 10 km/h - 40 km/h) and 50% longer\n",
    "        DD = df.day_delta[i]\n",
    "        if DD > 1 :\n",
    "            label.append(0)\n",
    "        elif (df.est_meters[i] < DIST_MIN) or (df.wait_sec[i] > WT): \n",
    "            label.append(0)\n",
    "        elif (df.dist_meters[i] > DLB) and (df.dist_meters[i] < DHB) :\n",
    "            if (df.trip_duration[i] > TLB) and (df.trip_duration[i] < THB) :\n",
    "                label.append(1)\n",
    "            else :\n",
    "                label.append(0)\n",
    "        else :\n",
    "            label.append(0)\n",
    "    return label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uber_df[\"label\"] = labeler(uber_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(uber_df[\"label\"].nunique()) :\n",
    "    print(\"label {} : {}\".format(i, list(uber_df[\"label\"].values).count(i)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(9, 7))\n",
    "sns.countplot(\n",
    "    x= \"label\",\n",
    "    data= uber_df\n",
    ")\n",
    "\n",
    "plt.title(\"Distribusi Label\",\n",
    "    loc=\"center\",\n",
    "    fontweight=\"bold\",\n",
    "    size=15\n",
    ")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Re-check Labeling Logic Reliability\n",
    "Cek kembali apakah hasil dari labeling sudah sesuai dengan landasan teori"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load only True data\n",
    "true_data = uber_df[(uber_df.label == 1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot dist_meter and trip_duratuion in hour\n",
    "plt.figure(figsize=(9, 7))\n",
    "sns.scatterplot(\n",
    "    data=true_data,\n",
    "    x=true_data.dist_meters / 1000,\n",
    "    y=true_data.trip_duration / 3600,\n",
    "    s=100,\n",
    "    alpha=0.85\n",
    ")\n",
    "plt.title(\"Durasi vs Jarak Tempuh Trip True\",\n",
    "    loc=\"right\",\n",
    "    fontweight=\"bold\",\n",
    "    size=15\n",
    ")\n",
    "plt.xlabel(\"km\")\n",
    "plt.ylabel(\"jam\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_data.avg_kmph.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot kmph distribution\n",
    "plt.figure(figsize=(9, 7))\n",
    "sns.violinplot(x=true_data.avg_kmph)\n",
    "\n",
    "plt.title(\n",
    "    \"Distribusi Kecepatan Rata-Rata Trip True\",\n",
    "    loc=\"right\",\n",
    "    fontweight=\"bold\",\n",
    "    size=15\n",
    ")\n",
    "plt.xlabel(\"km / jam\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot wait_sec distribution\n",
    "plt.figure(figsize=(9, 7))\n",
    "sns.violinplot(x=true_data[\"wait_sec\"] / 60)\n",
    "\n",
    "plt.title(\n",
    "    \"Distribusi Waktu Tunggu Driver True\",\n",
    "    loc=\"right\",\n",
    "    fontweight=\"bold\",\n",
    "    size=15\n",
    ")\n",
    "plt.xlabel(\"menit\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data yang valid sudah memenuhi kriteria pada landasan teori seperti kecepatan maksimal adalah 72 km/jam dibawah batas maksimal kecepatan tol dalam kota dan waktu maksimal driver untuk menunggu tidak lebih dari 5 menit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Missing Value Checking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_missing = uber_df.isnull().sum() * 100 / uber_df.shape[0]\n",
    "check_missing[check_missing > 0].sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tidak ada data yang hilang / tidak sesuai jadi tidak perlu ditindaklanjuti"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correlation Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(14,7))\n",
    "sns.heatmap(uber_df.corr(), annot=True, cmap=\"YlGnBu\", mask=np.triu(uber_df.corr()))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tidak ada feature yang berkorelasi sangat kuat terhadap label, tapi saya menawarkan untuk memilih selain *_longitude, dist_meters, avg_kmph dan diff_meters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_data = uber_df.drop([\"pickup_longitude\", \"dropoff_longitude\", \"dist_meters\", \"avg_kmph\", \"diff_meters\"], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FEATURE SCALING AND TRANSFORMATION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One Hot Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_cols = model_data.select_dtypes(include='object').columns.tolist()\n",
    "ohe = pd.get_dummies(model_data[categorical_cols])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ohe.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_data = pd.concat([model_data.drop(categorical_cols, axis=1), ohe], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train, X_valid, y_valid, X_test, y_test = train_valid_test_split(model_data, target = 'label', \n",
    "                                                                            train_size=0.6, valid_size=0.3, test_size=0.1, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Standardization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numerical_cols = [col for col in X_train.columns.tolist() if col not in ohe.columns.tolist() + ['label']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "X_train[numerical_cols] = scaler.fit_transform(X_train[numerical_cols])\n",
    "X_valid[numerical_cols] = scaler.fit_transform(X_valid[numerical_cols])\n",
    "X_test[numerical_cols] = scaler.fit_transform(X_test[numerical_cols])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Class Balancing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "oversample = SMOTETomek(random_state = 42, n_jobs= -1)\n",
    "X_train, y_train = oversample.fit_resample(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Machine Learning Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, f1_score, confusion_matrix, roc_curve, roc_auc_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deep Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set memory limiter for each GPU\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "    except RuntimeError as e:\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "earlystop = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    min_delta=0.01,\n",
    "    mode = \"min\",\n",
    "    patience=3,\n",
    "    verbose=1,\n",
    "    baseline=None,\n",
    "    restore_best_weights=True\n",
    ")\n",
    "\n",
    "csv_log = tf.keras.callbacks.CSVLogger(\n",
    "    os.path.join(\"model\", \"history.csv\"), \n",
    "    separator=\",\", \n",
    "    append=False\n",
    ")\n",
    "\n",
    "class mC(tf.keras.callbacks.Callback):\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        if(logs.get(\"val_accuracy\") >= 95):\n",
    "            self.model.stop_training = True\n",
    "limiter = mC()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.backend.clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "KR = tf.keras.regularizers.L2(\n",
    "    l2=0.001\n",
    ")\n",
    "\n",
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.LayerNormalization(),\n",
    "    tf.keras.layers.Dense(512, input_shape=(X_train.shape[1], ), activation=\"LeakyReLU\", kernel_regularizer=KR),\n",
    "    tf.keras.layers.Dropout(0.5),\n",
    "    tf.keras.layers.Dense(256, activation=\"LeakyReLU\", kernel_regularizer=KR),\n",
    "    tf.keras.layers.Dropout(0.5),\n",
    "    tf.keras.layers.Dense(128, activation=\"LeakyReLU\", kernel_regularizer=KR),\n",
    "    tf.keras.layers.Dropout(0.2),\n",
    "    tf.keras.layers.Dense(64, activation=\"LeakyReLU\", kernel_regularizer=KR),\n",
    "    tf.keras.layers.Dropout(0.2),\n",
    "    tf.keras.layers.Dense(1, activation=\"sigmoid\", kernel_regularizer=KR)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    optimizer=Adam(learning_rate=0.0001),\n",
    "    loss=\"binary_crossentropy\",\n",
    "    metrics = [\"accuracy\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    validation_data = (X_valid, y_valid),\n",
    "    batch_size=128,\n",
    "    validation_batch_size=32,\n",
    "    epochs=100, \n",
    "    verbose = 1,\n",
    "    callbacks = [earlystop, csv_log, limiter]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Analyze Deep Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc = history.history['accuracy']\n",
    "val_acc = history.history['val_accuracy']\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "\n",
    "epochs = range(len(acc))\n",
    "plt.figure(figsize=(9, 7))\n",
    "plt.plot(epochs, acc, 'r', label='Training accuracy')\n",
    "plt.plot(epochs, val_acc, 'b', label='Validation accuracy')\n",
    "plt.title('Training and validation accuracy')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(9, 7))\n",
    "plt.plot(epochs, loss, 'r', label='Training Loss')\n",
    "plt.plot(epochs, val_loss, 'b', label='Validation Loss')\n",
    "plt.title('Training and validation loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_pred = model.predict(X_valid)\n",
    "val_result = []\n",
    "for pred in val_pred :\n",
    "    if pred > 0.5 :\n",
    "        val_result.append(1)\n",
    "    else :\n",
    "        val_result.append(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(list(y_valid), val_result, target_names=[\"0\", \"1\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1 = f1_score(list(y_valid), val_result)\n",
    "print(f1 * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = confusion_matrix(list(y_valid), val_result)\n",
    "cm_norm = np.round(cm / np.sum(cm, axis=1).reshape(-1,1), 2)\n",
    "plt.figure(figsize=(9, 7))\n",
    "sns.heatmap(\n",
    "    cm_norm, \n",
    "    cmap=\"YlGnBu\", \n",
    "    annot=True\n",
    ")\n",
    "plt.title(\"Normalized Deep Neural Network Confusion Matrix\",\n",
    "    loc=\"center\",\n",
    "    fontweight=\"bold\",\n",
    "    size=15\n",
    ")\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"Actual\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfc = RandomForestClassifier(max_depth=3, random_state=42)\n",
    "rfc.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arr_feature_importances = rfc.feature_importances_\n",
    "arr_feature_names = X_train.columns.values\n",
    "    \n",
    "df_feature_importance = pd.DataFrame(index=range(len(arr_feature_importances)), columns=['feature', 'importance'])\n",
    "df_feature_importance['feature'] = arr_feature_names\n",
    "df_feature_importance['importance'] = arr_feature_importances\n",
    "df_all_features = df_feature_importance.sort_values(by='importance', ascending=False)\n",
    "df_all_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Analyze Random Forest (1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_proba = rfc.predict_proba(X_valid)[:][:,1]\n",
    "\n",
    "df_actual_predicted = pd.concat([pd.DataFrame(np.array(y_valid), columns=['y_actual']), pd.DataFrame(y_pred_proba, columns=['y_pred_proba'])], axis=1)\n",
    "df_actual_predicted.index = y_valid.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fpr, tpr, tr = roc_curve(df_actual_predicted['y_actual'], df_actual_predicted['y_pred_proba'])\n",
    "auc = roc_auc_score(df_actual_predicted['y_actual'], df_actual_predicted['y_pred_proba'])\n",
    "\n",
    "plt.figure(figsize=(9, 7))\n",
    "plt.plot(fpr, tpr, label='AUC = %0.4f' %auc)\n",
    "plt.plot(fpr, fpr, linestyle = '--', color='k')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Curve')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_actual_predicted = df_actual_predicted.sort_values('y_pred_proba')\n",
    "df_actual_predicted = df_actual_predicted.reset_index()\n",
    "\n",
    "df_actual_predicted['Cumulative N Population'] = df_actual_predicted.index + 1\n",
    "df_actual_predicted['Cumulative N Bad'] = df_actual_predicted['y_actual'].cumsum()\n",
    "df_actual_predicted['Cumulative N Good'] = df_actual_predicted['Cumulative N Population'] - df_actual_predicted['Cumulative N Bad']\n",
    "df_actual_predicted['Cumulative Perc Population'] = df_actual_predicted['Cumulative N Population'] / df_actual_predicted.shape[0]\n",
    "df_actual_predicted['Cumulative Perc Bad'] = df_actual_predicted['Cumulative N Bad'] / df_actual_predicted['y_actual'].sum()\n",
    "df_actual_predicted['Cumulative Perc Good'] = df_actual_predicted['Cumulative N Good'] / (df_actual_predicted.shape[0] - df_actual_predicted['y_actual'].sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_actual_predicted.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "KS = max(df_actual_predicted['Cumulative Perc Good'] - df_actual_predicted['Cumulative Perc Bad'])\n",
    "\n",
    "plt.figure(figsize=(9, 7))\n",
    "plt.plot(df_actual_predicted['y_pred_proba'], df_actual_predicted['Cumulative Perc Bad'], color='r')\n",
    "plt.plot(df_actual_predicted['y_pred_proba'], df_actual_predicted['Cumulative Perc Good'], color='b')\n",
    "plt.xlabel('Estimated Probability for Being Bad')\n",
    "plt.ylabel('Cumulative %')\n",
    "plt.title('Kolmogorov-Smirnov:  %0.4f' %KS)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Analyze Random Forest (2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_pred = rfc.predict(X_valid)\n",
    "val_result = []\n",
    "for pred in val_pred :\n",
    "    if pred > 0.5 :\n",
    "        val_result.append(1)\n",
    "    else :\n",
    "        val_result.append(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(list(y_valid), val_result, target_names=[\"0\", \"1\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1 = f1_score(list(y_valid), val_result)\n",
    "print(f1 * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = confusion_matrix(list(y_valid), val_result)\n",
    "cm_norm = np.round(cm / np.sum(cm, axis=1).reshape(-1,1), 2)\n",
    "plt.figure(figsize=(9, 7))\n",
    "sns.heatmap(\n",
    "    cm_norm, \n",
    "    cmap=\"YlGnBu\", \n",
    "    annot=True\n",
    ")\n",
    "plt.title(\"Normalized Random Forest Confusion Matrix\",\n",
    "    loc=\"center\",\n",
    "    fontweight=\"bold\",\n",
    "    size=15\n",
    ")\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"Actual\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def elbow() :\n",
    "    error_rate = []\n",
    "    for i in range(1,20):\n",
    "        knn = KNeighborsClassifier(n_neighbors=i, n_jobs=-1)\n",
    "        knn.fit(X_train,y_train)\n",
    "        pred_i = knn.predict(X_valid)\n",
    "        error_rate.append(np.mean(pred_i != y_valid))\n",
    "    return error_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(9, 7))\n",
    "plt.plot(range(1,20), elbow(), color=\"blue\", linestyle=\"dashed\", marker=\"o\",\n",
    "    markerfacecolor=\"red\", markersize=10)\n",
    "plt.title(\"Error Rate vs. K Value\")\n",
    "plt.xlabel(\"K\")\n",
    "plt.ylabel(\"Error Rate\")\n",
    "plt.xlim([0,20])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn = KNeighborsClassifier(n_neighbors=2, n_jobs=-1)\n",
    "knn.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Analyze KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_pred = knn.predict(X_valid)\n",
    "val_result = []\n",
    "for pred in val_pred :\n",
    "    if pred > 0.5 :\n",
    "        val_result.append(1)\n",
    "    else :\n",
    "        val_result.append(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(list(y_valid), val_result, target_names=[\"0\", \"1\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1 = f1_score(list(y_valid), val_result)\n",
    "print(f1 * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = confusion_matrix(list(y_valid), val_result)\n",
    "cm_norm = np.round(cm / np.sum(cm, axis=1).reshape(-1,1), 2)\n",
    "plt.figure(figsize=(9, 7))\n",
    "sns.heatmap(\n",
    "    cm_norm, \n",
    "    cmap=\"YlGnBu\", \n",
    "    annot=True\n",
    ")\n",
    "plt.title(\"Normalized KNN Confusion Matrix\",\n",
    "    loc=\"center\",\n",
    "    fontweight=\"bold\",\n",
    "    size=15\n",
    ")\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"Actual\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logreg = LogisticRegression(random_state=42)\n",
    "logreg.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Analyze Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_pred = logreg.predict(X_valid)\n",
    "val_result = []\n",
    "for pred in val_pred :\n",
    "    if pred > 0.5 :\n",
    "        val_result.append(1)\n",
    "    else :\n",
    "        val_result.append(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(list(y_valid), val_result, target_names=[\"0\", \"1\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1 = f1_score(list(y_valid), val_result)\n",
    "print(f1 * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = confusion_matrix(list(y_valid), val_result)\n",
    "cm_norm = np.round(cm / np.sum(cm, axis=1).reshape(-1,1), 2)\n",
    "plt.figure(figsize=(9, 7))\n",
    "sns.heatmap(\n",
    "    cm_norm, \n",
    "    cmap=\"YlGnBu\", \n",
    "    annot=True\n",
    ")\n",
    "plt.title(\"Normalized Logistic Regression Confusion Matrix\",\n",
    "    loc=\"center\",\n",
    "    fontweight=\"bold\",\n",
    "    size=15\n",
    ")\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"Actual\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TEST DATA TIME!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DNN_test_predict = model.predict(X_test)\n",
    "DNN_test_preds = []\n",
    "for pred in DNN_test_predict :\n",
    "    if pred > 0.5 :\n",
    "        DNN_test_preds.append(1)\n",
    "    else :\n",
    "        DNN_test_preds.append(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RFC_test_predict = rfc.predict(X_test)\n",
    "RFC_test_preds = []\n",
    "for pred in RFC_test_predict :\n",
    "    if pred > 0.5 :\n",
    "        RFC_test_preds.append(1)\n",
    "    else :\n",
    "        RFC_test_preds.append(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "KNN_test_predict = knn.predict(X_test)\n",
    "KNN_test_preds = []\n",
    "for pred in KNN_test_predict :\n",
    "    if pred > 0.5 :\n",
    "        KNN_test_preds.append(1)\n",
    "    else :\n",
    "        KNN_test_preds.append(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LOGREG_test_predict = logreg.predict(X_test)\n",
    "LOGREG_test_preds = []\n",
    "for pred in LOGREG_test_predict :\n",
    "    if pred > 0.5 :\n",
    "        LOGREG_test_preds.append(1)\n",
    "    else :\n",
    "        LOGREG_test_preds.append(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"f1 score Deep Neural Network : {round(f1_score(list(y_test), DNN_test_preds), 4) * 100}%\")\n",
    "print(f\"f1 score Random Forest Classifier : {round(f1_score(list(y_test), RFC_test_preds), 4) * 100}%\")\n",
    "print(f\"f1 score Logistic Regression : {round(f1_score(list(y_test), LOGREG_test_preds), 4) * 100}%\")\n",
    "print(f\"f1 score K-Nearest Neighbours : {round(f1_score(list(y_test), KNN_test_preds), 4) * 100}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = confusion_matrix(list(y_test), DNN_test_preds)\n",
    "cm_norm = np.round(cm / np.sum(cm, axis=1).reshape(-1,1), 2)\n",
    "plt.figure(figsize=(9, 7))\n",
    "sns.heatmap(\n",
    "    cm_norm, \n",
    "    cmap=\"YlGnBu\", \n",
    "    annot=True\n",
    ")\n",
    "plt.title(\"TEST - Normalized Deep Neural Network Confusion Matrix\",\n",
    "    loc=\"center\",\n",
    "    fontweight=\"bold\",\n",
    "    size=15\n",
    ")\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"Actual\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = confusion_matrix(list(y_test), RFC_test_preds)\n",
    "cm_norm = np.round(cm / np.sum(cm, axis=1).reshape(-1,1), 2)\n",
    "plt.figure(figsize=(9, 7))\n",
    "sns.heatmap(\n",
    "    cm_norm, \n",
    "    cmap=\"YlGnBu\", \n",
    "    annot=True\n",
    ")\n",
    "plt.title(\"TEST - Normalized Random Forest Confusion Matrix\",\n",
    "    loc=\"center\",\n",
    "    fontweight=\"bold\",\n",
    "    size=15\n",
    ")\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"Actual\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = confusion_matrix(list(y_test), LOGREG_test_preds)\n",
    "cm_norm = np.round(cm / np.sum(cm, axis=1).reshape(-1,1), 2)\n",
    "plt.figure(figsize=(9, 7))\n",
    "sns.heatmap(\n",
    "    cm_norm, \n",
    "    cmap=\"YlGnBu\", \n",
    "    annot=True\n",
    ")\n",
    "plt.title(\"TEST - Normalized Logistic Regression Confusion Matrix\",\n",
    "    loc=\"center\",\n",
    "    fontweight=\"bold\",\n",
    "    size=15\n",
    ")\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"Actual\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = confusion_matrix(list(y_test), KNN_test_preds)\n",
    "cm_norm = np.round(cm / np.sum(cm, axis=1).reshape(-1,1), 2)\n",
    "plt.figure(figsize=(9, 7))\n",
    "sns.heatmap(\n",
    "    cm_norm, \n",
    "    cmap=\"YlGnBu\", \n",
    "    annot=True\n",
    ")\n",
    "plt.title(\"TEST - Normalized KNN Confusion Matrix\",\n",
    "    loc=\"center\",\n",
    "    fontweight=\"bold\",\n",
    "    size=15\n",
    ")\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"Actual\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kesimpulan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Dapat disimpulkan model **Deep Neural Network** lebih andal dengan **f1 score : 56.37%** pada data test dibandingkan dengan model lainnya. Dengan tingkat false positive yang cukup rendah 0.18 dan false negative 0.42, model dapat diandalkan karena **lebih sedikit kesalahan** memprediksi perjalanan yang **sebenarnya valid dan tidak valid**.\n",
    "\n",
    "* Sebagai alternatif, model **Random Forest Classifier** memiliki **f1 score : 54.42%** pada data test. Dengan pemodelan yang lebih ringkas namun performanya tidak jauh berbeda dengan Deep Neural Network. Model ini menghasilkan **false positive yang lebih sedikit**, yang berarti akan **minim kesalahan penagihan** dan false negative dapat diverifikasi kembali total tagihan yang harus dibayar oleh customer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!jupyter-nbconvert --to PDFviaHTML uber_trip_classification.ipynb"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('directml310')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e8a583a2b953fbef8a75c843262eec54989b605207a7969c58ae2c4ad060d122"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
